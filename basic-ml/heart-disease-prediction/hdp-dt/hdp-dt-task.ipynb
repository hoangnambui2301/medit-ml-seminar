{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc337c-ad48-4656-ade0-ea39bcead6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries and tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad602191-5865-46e8-955e-c308355c47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a first look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289c1bb-67e8-44d0-b460-6ca5968c6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515536ab-d66c-411e-8629-e2e8bb7594b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a418e-7439-45af-84db-00ae697e8fc5",
   "metadata": {},
   "source": [
    "### Classification of data types\n",
    "Before going to any training, we should classify the types of data into two different kinds: 'categorical_val' for whose the unique data is less than 10 different values (e.g. age, sex...) and 'continuous_val' vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e3c59-8213-48a4-bd10-ed01bc97171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill your answer in '...'\n",
    "categorical_val = []\n",
    "continuous_val = []\n",
    "for col in ...:\n",
    "    if ...:\n",
    "        ...\n",
    "    else:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3511b-3cc8-46a6-ba65-81c75ced6a6d",
   "metadata": {},
   "source": [
    "### Create dummies and scale data\n",
    "After exploring the dataset, we need to convert some categorical variables into dummy variables and scale all the values before training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95baba-eca8-4118-b666-0f4444a2288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dummies\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "categorical_val.remove('target')\n",
    "dataset = pd.get_dummies(..., columns=...)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93852af5-7a50-42a7-b7ad-660d85925bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scale the values\n",
    "- Set the array of columns to scale.\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "s_sc = StandardScaler()\n",
    "col_to_scale = ...\n",
    "dataset[col_to_scale] = s_sc.fit_transform(dataset[col_to_scale])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b479d7d-054c-45f2-96f3-cedc9f0552ed",
   "metadata": {},
   "source": [
    "#### Define function to print the accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df7885-7c9c-42b8-be00-f998d1f88743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def print_score(clf, x_train, y_train, x_test, y_test, train):\n",
    "    if train == True:\n",
    "        pred = clf.predict(x_train)\n",
    "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
    "        print(f'Accuracy Score: {accuracy_score(y_train, pred) * 100:.4f}%')\n",
    "        print('______________________________________________________________________')\n",
    "        print(f'Classification Report:\\n{clf_report}')\n",
    "#         print('______________________________________________________________________')\n",
    "#         print(f'Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n')\n",
    "    elif train == False:\n",
    "        pred = clf.predict(x_test)\n",
    "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
    "        print(f'Accuracy Score: {accuracy_score(y_test, pred) * 100:.4f}%')\n",
    "        print('______________________________________________________________________')\n",
    "        print(f'Classification Report:\\n{clf_report}')\n",
    "#         print('______________________________________________________________________')\n",
    "#         print(f'Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e085a-58fd-4320-bd54-5ccd8b35b489",
   "metadata": {},
   "source": [
    "### Dataset splitting\n",
    "Split the dataset into training (70%) and test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c4817-5d7b-46e2-a8a0-a27516bf5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use train_test_split(data, target, test_size, random)\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.drop('target', axis=1)\n",
    "y = dataset.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(..., ..., test_size=..., random_state=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e540a-323b-41bf-bb3f-1d55a8a1770e",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Decision Trees are a non-parametric supervised learning method used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Let's look at an simple example:\n",
    "\n",
    "| ![](https://miro.medium.com/max/450/1*XMId5sJqPtm8-RIwVVz2tg.png) |\n",
    "|:--:|\n",
    "| <b>Example of decision tree.</b> |\n",
    "\n",
    "In the image, the bold texts represent **internal nodes** (a.k.a. nodes or conditions). The **edges/branches** then split based on these nodes. If we can not branch anymore , then those nodes are called **decision** (or leaf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47623b-8ff3-4cc9-b969-826183ebfdf8",
   "metadata": {},
   "source": [
    "### Backgorund algorithm\n",
    "In order to decide which features to choose and what conditions to use for splitting, along with knowing when to stop, we need to calculate how much *accuracy* eah split (branch) will cost us, using the function called cost function. **The split costs least is chosen**. This algorthim is also known as **greedy algorithm** as we have an excessive desire of lowering the cost. This makes the root node as best predictor (for regression problems) or classifier (for classification problems). In both cases the cost functions try to find most homogeneous branches, or branches having groups with similar responses.\n",
    "- For the regression problem, we can use a simple squred error:\n",
    "\n",
    "$cost=\\sum(y-pred)^2=\\sum(y-\\hat{y})^2$\n",
    "\n",
    "where $y$ is the ground truth and $\\hat{y}$ is the predicted value.\n",
    "\n",
    "- For the classification problem, we use the *Gini index function*:\n",
    "\n",
    "$cost=\\sum(p_k(1-p_k))$\n",
    "\n",
    "where $p_k$ is the proportion of training instances of class k in a particular prediction node. What we want here is that a particular decision node should *ideally* have an error value of 0, which means that each split outputs a single class 100% of the time.\n",
    "The concept of having a single class per split is also known as *information gain*. Shortly, we won't gain any information if we have to choose a split where each output has a mix of classes. On the other hand, if our split has a high percentage of each class for each output, then we have gained the information that splitting in that particular way on that particular feature variable gives us a particular output. The information gain can be intuitively understanded as the image below:\n",
    "\n",
    "| ![](https://miro.medium.com/max/750/1*z7tK94rGGIy_42UpiqilLQ.png) |\n",
    "|:--:|\n",
    "| <b>Intuition of information gain</b> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205367a-c31f-4456-b668-8b36a12e015f",
   "metadata": {},
   "source": [
    "The nature of decision tree can expand the tree so bad, indeed many of these splits will end up being redundant and unnecessary to increasing the accuracy of our model. Thus, we need to manage some stopping criterion to halt the construction of the tree. Here we have a common solution, **tree pruning**. It is a technique that leverages this splitting redundancy to remove i.e *prune* the unnecessary splits in our tree.\n",
    "\n",
    "| ![](https://miro.medium.com/max/848/1*TxzPx2UmUdhKieWruQ1prA.png) |\n",
    "|:--:|\n",
    "| <b>Tree pruning</b> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f660af-bda4-4c69-9523-0ab3a76fa13d",
   "metadata": {},
   "source": [
    "## Build a decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0622cde-7a9c-47e0-a34f-ab7141b8ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building a decision model with Scitkit Learn\n",
    "- set random_state to 42\n",
    "- fit the training data\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfec2bb-3134-4e78-a45a-decb72de907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(dt_clf, x_train, y_train, x_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90488fa4-6531-4089-9a18-4828a3436491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_score(dt_clf, x_train, y_train, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a64f0-bfa8-49b9-9eb9-387c5f67eba3",
   "metadata": {},
   "source": [
    "## Improve model with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c04c7-9932-4cfe-8b06-f5fef8bf82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement some different hyperparameters to test their efficacy on new decision tree model: \n",
    "- criterion: we aim to calculate 'gini' and 'entropy' index\n",
    "- splitter: use 'best' and 'random'\n",
    "- max_depth: list of number from 1 to 20\n",
    "- min_samples_split: array of 2, 3, 4 \n",
    "  'min_samples_leaf': list of number from 1 to 20\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {...}\n",
    "# dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_cv = GridSearchCV(..., ..., scoring='accuracy', n_jobs=-1, verbose=1, cv=3).fit(x_train, y_train)\n",
    "best_params = dt_cv.best_params_\n",
    "print(f'Best_params: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f03032-b468-40bb-90cc-569ba0dff997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best parameters to the decision tree model\n",
    "dt_clf_tuning = DecisionTreeClassifier(**best_params).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6e492-a969-4941-84b6-bd7e5878cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(dt_clf_tuning, x_train, y_train, x_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd7e69-2c77-4f3b-b437-d20c2394a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(dt_clf_tuning, x_train, y_train, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2f358-3ab1-4588-83ed-4421d0a784ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summarize the accuracy score of decision tree algorithm with and without hyperparameters.\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "dt_clf_train = accuracy_score(y_train, dt_clf.predict(x_train)) * 100\n",
    "dt_clf_test = accuracy_score(y_test, dt_clf.predict(x_test)) * 100\n",
    "dt_clf_tuning_train = ...\n",
    "dt_clf_tuning_test = ...\n",
    "\n",
    "result = pd.DataFrame(columns=['Model', 'Non-tuning train accuracy %', 'Tuning train accuracy %', 'Non-tuning test accuracy %', 'Tuning test accuracy %'])\n",
    "dt_result = pd.DataFrame(data=[['Decision Tree', ..., ..., ..., ...]],\n",
    "                        columns=['Model', 'Non-tuning train accuracy %', 'Tuning train accuracy %', 'Non-tuning test accuracy %', 'Tuning test accuracy %'])\n",
    "result = result.append(dt_result, ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda5470-4edc-4ade-b051-1341fb3eddbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
