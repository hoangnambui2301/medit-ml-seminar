{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951f80f8-c643-4b28-be27-6b6926b031c4",
   "metadata": {},
   "source": [
    "# Retinal vessel segmentation\n",
    "Retinal vessel segmentation is the task of segmenting blood vessels in retinal images. It is an essential task for developing the computer-aided diagnosis system for retinal diseases.\n",
    "\n",
    "| ![](https://github.com/orobix/retina-unet/raw/master/test/test_Original_GroundTruth_Prediction3.png) |\n",
    "|:--:|\n",
    "| <b>Retinal vessel segmentation</b> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794f681-6b7c-4b63-9433-a55fefc9e9bd",
   "metadata": {},
   "source": [
    "# U-Net architecture\n",
    "| ![](https://camo.githubusercontent.com/bc2e09476b5c7db5ea4e19251ac9a19af9ba5a89f16d58f72459059c3cffb969/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a6a716f416d4579516d784b704763416b6250474e4d512e706e67) |\n",
    "|:--:|\n",
    "| <b>U-Net model</b> |\n",
    "\n",
    "The U-Net is convolutional network architecture for fast and precise segmentation of images. It is an encoder-decoder model with some skip connections between. The major advantage of this architecture is its ability to take into account a wider context when making a prediction for a pixel (foreground vs. background). This model consists of large number of channels used in the up-sampling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8855fdaf-399f-4ea8-b6f9-6374a5ff536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "import os\n",
    "import numpy as np\n",
    "import configparser\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Keras\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676ab928-ec7a-4adb-a2a7-31ce43a9f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "training_1st_manual = './DRIVE/training/1st_manual/'\n",
    "training_images = './DRIVE/training/images/'\n",
    "training_masks = './DRIVE/training/mask/'\n",
    "test_images = './DRIVE/test/images/'\n",
    "test_masks = './DRIVE/test/mask/'\n",
    "\n",
    "# Dimensions of patch\n",
    "patch_height = 48\n",
    "patch_width = 48\n",
    "\n",
    "# Training settings\n",
    "n_epochs = 150\n",
    "batch_size = 32\n",
    "n_subimgs = 190000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0c496-61de-4029-9531-00d4ed81fa69",
   "metadata": {},
   "source": [
    "## 1. Pre-processing images\n",
    "Before training, the 20 images of DRIVE training datasets are pre-processed with the following steps:\n",
    "- Gray-scale conversion\n",
    "- Normalization\n",
    "- Contrast-limited adaptive historam equalization (CLAHE)\n",
    "- Gamma adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4c4fa5-2f3e-4ff3-9691-bf9bf2a7ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gray-scale conversion\n",
    "def rgb2gray(rgb_imgs):\n",
    "    assert len(rgb_imgs.shape) == 4 # 4D arrays\n",
    "    assert rgb_imgs.shape[1] == 3 # Check the original RGB images\n",
    "    \n",
    "    bn_imgs = rgb_imgs[:,0,:,:]*0.299 + rgb_imgs[:,1,:,:]*0.587 + _rgb_imgs[:,2,:,:]*0.114\n",
    "    bn_imgs = np.reshape(bn_imgs,(rgb_imgs.shape[0], 1, rgb_imgs.shape[2], rgb_imgs.shape[3]))\n",
    "    \n",
    "    return bn_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2399b4-8b4b-4192-8d31-a1c3672b933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def dataset_normalize(imgs):\n",
    "    assert len(imgs.shape) == 4 # 4D arrays\n",
    "    assert imgs.shape[1] == 1 # Check the gray-scale images\n",
    "    \n",
    "    # Normalize mean and standard deviation of the images\n",
    "    normalized_imgs = np.empty(imgs.shape)\n",
    "    normalized_imgs = (imgs-np.mean(imgs))/np.std(imgs)\n",
    "    \n",
    "    for i in range(imgs.shape[0]):\n",
    "        normalized_imgs[i] = ((normalized_imgs[i]-np.min(normalized_imgs[i]))/(np.max(normalized_imgs[i])-np.min(normalized_imgs[i])))*255\n",
    "        \n",
    "        return normalized_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb7e71b-5db0-4f3e-8a7e-c9904a5990b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram equalization\n",
    "def his_equalized(imgs):\n",
    "    assert len(imgs.shape) == 4 # 4D arrays\n",
    "    assert imgs.shape[1] == 1 # Check the gray-scale images\n",
    "    \n",
    "    equalized_imgs = np.empty(imgs.shape)\n",
    "    \n",
    "    for i in range(imgs.shape[0]):\n",
    "        equalized_imgs[i,0] = cv2.equalizeHist(np.array(imgs[i,0], dtype=np.uint8))\n",
    "    \n",
    "    return equalized_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ece708-8d94-4505-819a-eaa43e6c6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrast-limited adaptive histogram equalization (CLAHE)\n",
    "def clahe(imgs):\n",
    "    assert len(imgs.shape) == 4 # 4D arrays\n",
    "    assert imgs.shape[1] == 1 # Check the gray-scale images\n",
    "    \n",
    "    # Create a CLAHE object\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    clahe_imgs = np.empty(imgs.shape)\n",
    "    \n",
    "    for i in range(imgs.shape[0]):\n",
    "        clahe_imgs[i,0] = clahe.apply(np.array(imgs[i,0], dtype=np.uint8))\n",
    "    \n",
    "    return clahe_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1e0c86-64a5-4c20-a48b-3808325c97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma adjustment\n",
    "def gamma_adjust(imgs, gamma):\n",
    "    assert len(imgs.shape) == 4 # 4D arrays\n",
    "    assert imgs.shape[1] == 1 # Check the gray-scale images\n",
    "    \n",
    "    # Build a lookup table mapping the pixel values to their adjusted gamma values\n",
    "    inv_gamma = 1.0/gamma\n",
    "    table = np.array([((i/255)**inv_gamma) for i in np.arange(0,256)]).astype('uint8')\n",
    "    \n",
    "    # Apply gamma adjustment using the lookup table\n",
    "    adjusted_imgs = np.array(imgs.shape)\n",
    "    \n",
    "    for i in range(imgs.shape[0]):\n",
    "        adjusted_imgs[i,0] = cv2.LUT(np.array(imgs[i,0], dtype=np.uint8), table)\n",
    "        \n",
    "    return adjusted_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8760b1d6-5140-46d3-8013-c2be9fe61e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "def pre_process(original_imgs):\n",
    "    assert len(original_imgs.shape) == 4 # 4D arrays\n",
    "    assert original_imgs.shape[1] == 1 # Check the original RGB images\n",
    "    \n",
    "    train_imgs = rgb2gray(original_imgs)\n",
    "    train_imgs = dataset_normalize(train_imgs)\n",
    "    train_imgs = clahe(train_imgs)\n",
    "    train_imgs = gamma_adjust(train_imgs, 1.2)\n",
    "    train_imgs = train_imgs/255.\n",
    "    \n",
    "    return train_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2db8c-6460-4371-8d95-8859adde8aca",
   "metadata": {},
   "source": [
    "## 2. Extract image patches\n",
    "The training of U-Net model is performed on sub-images (called patches of dimension 48x48) of the pre-processed images. Each patch has the randomly selected center in the full image as well as the partially or completely outside the field of view (FOV). In this way the neural network learns how to discriminate the FOV border from blood vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6775ab79-82f9-4d47-b2e9-62891e011026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the patch is fully contained in the FOV\n",
    "def is_patch_inside_FOV(x, y, img_width, img_height, patch_height):\n",
    "    x_ = x-int(img_width/2) # Origin (0,0) is shifted to image center\n",
    "    y_ = y-int(img_height/2)  # Origin (0,0) is shifted to image center\n",
    "    \n",
    "    # The limit to contain the full patch in the FOV\n",
    "    R_inside = 270-int((patch_height*np.sqrt(2.0))/2) # Radius is 270 (from DRIVE docs), minus the patch diagonal (assumed it is a square)\n",
    "    radius = np.sqrt((x_*x_)+(y_*y_))\n",
    "    if radius < R_inside:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c74b39a-5502-4259-a14d-396c6c0dabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from original images\n",
    "def extract_patches(original_imgs, masks, patch_height, patch_width, n_patches, inside=True):\n",
    "    if n_patches%original_imgs.shape[0] != 0:\n",
    "        exit()\n",
    "    assert len(original_imgs.shape) == 4 and len(masks.shape) == 4 # 4D arrays\n",
    "    assert original_imgs.shape[1] == 1 or original_imgs.shape[1] == 3\n",
    "    assert masks.shape[1] == 1\n",
    "    \n",
    "    patches = np.empty((n_patches, original_imgs.shape[1], patch_height, patch_width))\n",
    "    mask_patches = np.empty((n_patches, masks.shape[1], patch_height, patch_width))\n",
    "    img_height = original_imgs.shape[2]\n",
    "    img_width = original_imgs.shape[3]\n",
    "    \n",
    "    n_patches_per_img = int(n_patches/original_imgs.shape[0])\n",
    "    print('Number of patches per full original image: ' + str(n_patches_per_img))\n",
    "    \n",
    "    iter_total = 0 # Total iterations of patch through the original images\n",
    "    for i in range(original_imgs.shape[0]): # Loop over full original images\n",
    "        k = 0\n",
    "        x_center = random.randint(0+int(patch_width/2), img_width-int(patch_width/2))\n",
    "        y_center = random.randint(0+int(patch_height/2), img_height-int(patch_height/2)) \n",
    "        \n",
    "        if inside == True:\n",
    "            if is_patch_inside_FOV(x_center, y_center, img_width, img_height, patch_height) == False:\n",
    "                continue\n",
    "            patch = original_imgs[i, :, y_center-int(patch_height/2):y_center+int(patch_height/2), x_center-int(patch_width/2):x_center+int(patch_width/2)]\n",
    "            mask_patch = masks[i, :, y_center-int(patch_height/2):y_center+int(patch_height/2), x_center-int(patch_width/2):x_center+int(patch_width/2)]\n",
    "            patches[iter_total] = patch\n",
    "            mask_patches[iter_total] = mask_patch\n",
    "            iter_total += 1   # Go to next iteration\n",
    "            k += 1\n",
    "            \n",
    "    return patches, mask_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c57734-bea5-47ba-9414-b05d5a5fe197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
