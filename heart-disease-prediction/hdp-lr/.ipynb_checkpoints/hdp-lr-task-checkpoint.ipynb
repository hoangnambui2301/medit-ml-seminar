{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc337c-ad48-4656-ade0-ea39bcead6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries and tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad602191-5865-46e8-955e-c308355c47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a first look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d2e10-208d-4767-a452-ec7da4ab6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890588b-1bcc-4af5-a5c4-d5ba00f69ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1021aa-c311-432b-acba-9d019973b301",
   "metadata": {},
   "source": [
    "### Classification of data types\n",
    "Before going to any training, we should classify the types of data into two different kinds: 'categorical_val' for whose the unique data is less than 10 different values (e.g. age, sex...) and 'continuous_val' vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e3c59-8213-48a4-bd10-ed01bc97171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill your answer in '...'\n",
    "categorical_val = []\n",
    "continuous_val = []\n",
    "for col in ...:\n",
    "    if ...:\n",
    "        ...\n",
    "    else:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fbc81-4ec7-40c6-9b96-38b9efee9e7c",
   "metadata": {},
   "source": [
    "### Create dummie and scale data\n",
    "After exploring the dataset, we need to convert some categorical variables into dummy variables and scale all the values before training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95baba-eca8-4118-b666-0f4444a2288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dummies\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "categorical_val.remove('target')\n",
    "dataset = pd.get_dummies(..., columns=...)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93852af5-7a50-42a7-b7ad-660d85925bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scale the values\n",
    "- Set the array of columns to scale.\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "s_sc = StandardScaler()\n",
    "col_to_scale = ...\n",
    "dataset[col_to_scale] = s_sc.fit_transform(dataset[col_to_scale])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d60eff-92e4-4910-8240-cc5c0c8ed1bc",
   "metadata": {},
   "source": [
    "#### Define function to print the accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df7885-7c9c-42b8-be00-f998d1f88743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def print_score(clf, x_train, y_train, x_test, y_test, train):\n",
    "    if train == True:\n",
    "        pred = clf.predict(x_train)\n",
    "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
    "        print(f'Accuracy Score: {accuracy_score(y_train, pred) * 100:.4f}%')\n",
    "        print('______________________________________________________________________')\n",
    "        print(f'Classification Report:\\n{clf_report}')\n",
    "#         print('______________________________________________________________________')\n",
    "#         print(f'Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n')\n",
    "    elif train == False:\n",
    "        pred = clf.predict(x_test)\n",
    "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
    "        print(f'Accuracy Score: {accuracy_score(y_test, pred) * 100:.4f}%')\n",
    "        print('______________________________________________________________________')\n",
    "        print(f'Classification Report:\\n{clf_report}')\n",
    "#         print('______________________________________________________________________')\n",
    "#         print(f'Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8dc33-94fb-4e2c-8f91-1d826b9a9587",
   "metadata": {},
   "source": [
    "### Dataset splitting\n",
    "Split the dataset into training (70%) and test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c4817-5d7b-46e2-a8a0-a27516bf5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use train_test_split(data, target, test_size, random)\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.drop('target', axis=1)\n",
    "y = dataset.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(..., ..., test_size=..., random_state=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c93cd-4b12-4fb1-a3ae-08df280744f3",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic regression (LR) is a classification algorithm used to assign observations to a discrete set of classes. It is a predictive analysis algorithm and based on the concept of probability. Its hypothesis tends it to limit the cost function between 0 and 1: \n",
    "\n",
    "$0\\leq{h_0}\\leq1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ccf88-66f7-4d27-a0b8-2d905c0328ac",
   "metadata": {},
   "source": [
    "### 1. Sigmoid function\n",
    "Sigmoid function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities. Formula of sigmoid function: \n",
    "\n",
    "$f(x)=\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe59e47-87f0-4505-a4ef-7b0c3ad49e3d",
   "metadata": {},
   "source": [
    "### 2. Decision boundary\n",
    "We expect our classifier to give us a set of outputs or classes based on probability when we pass the inputs through the prediction function (in this case sigmoid function) and returns a probability score between 0 and 1. Basically we decide a threshold value which we classify the probabilities into their correspond classes. For example:\n",
    "\n",
    "| ![](https://miro.medium.com/max/619/1*2Vsum532aNQX9TgR7_rAzQ.png) |\n",
    "|:--:|\n",
    "| <b>Decision boundary</b> |\n",
    "\n",
    "Let's think about a binary classification problem. In this fugure, the threshold value is set to be 0.5. Any probabilities that are higher than 0.5 are classified to class 1, otherwise they are classified into class 0. In real cases the threshold value varies from 0.5 to 0.7, depending on the types of data and problem requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67303f73-f622-4664-ad62-672d4e59507a",
   "metadata": {},
   "source": [
    "### 3. Cost function\n",
    "The cost function represents optimization objective i.e. we create a cost function and minimize it so that we can develop an accurate model with minimum error. For logistic regression, the Cost function is defined as:\n",
    "\n",
    "$J(\\theta)=\\begin{cases}-log(h_0(x))&(y=1)\\\\-log(1-h_0(x))&(y=0)\\end{cases}$\n",
    "\n",
    "| ![](https://miro.medium.com/max/609/1*yWzKLQhWITQ4bR2aMSVVuw.png) |\n",
    "|:--:|\n",
    "| <b>Intuition of logistic regression cost function.</b> |\n",
    "\n",
    "Or we can combine both equations in term of:\n",
    "\n",
    "$J(\\theta)=\\frac{-1}{m}\\sum(y^{(i)}log(h_0(x^{(i)})+(1-y^{(i)})log(1-h_0(x^{(i)}))$\n",
    "\n",
    "The final purpose of algorithm is to converge the cost function into the minimum error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5720e1-c84e-4156-abf4-956e055650e7",
   "metadata": {},
   "source": [
    "### 4. Gradient descent\n",
    "The problem of minimizing the cost value ($minJ(\\theta)$) can be done by using **gradient descent**. It is simply understanded that we \"update\" the value $\\theta$ after each training and use this $\\theta$ for the next training. This progress will be ended once the cost value $J(\\theta)$ **converge**. Mathematically:\n",
    "\n",
    "$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$\n",
    "\n",
    "Let's imagine that we are at the top of a mountain. We need to find the way to the valley while blindfolded. Feeling the slope of the terrain around you is all the things you can do. And it's actually analogous with the calculation of gradient descent, with the valley equals to the local minimum. \n",
    "\n",
    "| ![](https://media-exp1.licdn.com/dms/image/C5612AQGjsDROHG6DhQ/article-inline_image-shrink_1000_1488/0/1544817106951?e=1633564800&v=beta&t=rdLLJTl6sl9JoRlLgZs7DEu8TEn_6BzaCybiH5sEsbA) |\n",
    "|:--:|\n",
    "| <b>Gradient descent analogy. You start at the top of the hill. After each step of calculating, you approach closer and closer to the local minimum as \"valley\".</b> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c93facc-d4c1-4e50-8020-ac74fb0bd906",
   "metadata": {},
   "source": [
    "## Training logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0622cde-7a9c-47e0-a34f-ab7141b8ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training logistic regression model with Scikit Learn\n",
    "- solver: 'liblinear' (algorithm to use in the optimization)\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(solver=...).fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762354a2-3da1-4cfc-a2c3-bdbf8ac272c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(lr_clf, x_train, y_train, x_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e3f99-faff-4150-b2ef-48e969efd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(lr_clf, x_train, y_train, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5151004-88b3-46ff-a4e9-a35a7541bec0",
   "metadata": {},
   "source": [
    "## Improve logistic regression model with hyperparameter tuning\n",
    "In order to imporve the accuracy of the model and prevent overfitting, we add an additional parameter called **regularization** to our cost function. It plays a role as a *hyperparameter* for the cost function.\n",
    "#### (?) What is overfitting (and underfitting)?\n",
    "Overfitting occur when your model performance perfectly fits the data but poorly represent the whole dataset. Similarly, underfitting can be defined when your model are too weak to explain the variance of dataset, causing by insufficient data. The intuition of overfitting and underfitting can be explained by graphs:\n",
    "\n",
    "| ![](https://miro.medium.com/max/1400/1*_7OPgojau8hkiPUiHoGK_w.png) |\n",
    "|:--:|\n",
    "| <b>Model performance</b> |\n",
    "\n",
    "Back to the regularization. We implement the regularization hyperparameter into the cost function as the formula:\n",
    "\n",
    "$J(\\theta)=\\frac{-1}{m}\\sum(y^{(i)}log(h_0(x^{(i)})+(1-y^{(i)})log(1-h_0(x^{(i)}))+\\frac{\\lambda}{2}||w||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97f785-1ed0-4eeb-9ef9-628aa93c0684",
   "metadata": {},
   "source": [
    "##### Next, we will implement the regularization into our logistic regression using Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c124a-0e56-4feb-b186-b60364470785",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameter used: \n",
    "- C: inverse of regularization strength (set C value to be the array of numbers spaced evenly on a log scale)\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'C': ...(-4, 4, 20), 'solver': ['liblinear']}\n",
    "lr_cv = GridSearchCV(LogisticRegression(), params, scoring='accuracy', n_jobs=1, verbose=1, cv=5)\n",
    "lr_cv.fit(x_train, y_train)\n",
    "lr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df92e5-fe05-415b-9e17-87c3abbbcdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf_tuning = LogisticRegression(**lr_cv.best_params_).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeaceb5-04ec-408b-8d57-29e3aafd5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(lr_clf_tuning, x_train, y_train, x_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11b13b-7439-4726-baad-4f145d6f000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(lr_clf_tuning, x_train, y_train, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3d0f8-f1d0-4978-9258-5c880714f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summarize the accuracy score of logistic regression models with and without hyperparameter tuning\n",
    "'''\n",
    "# Please fill your answer in '...'\n",
    "lr_clf_train = accuracy_score(y_train, lr_clf.predict(x_train)) * 100\n",
    "lr_clf_test = accuracy_score(y_test, lr_clf.predict(x_test)) * 100\n",
    "lr_clf_tuning_train = ...\n",
    "lr_clf_tuning_test = ...\n",
    "\n",
    "result = pd.DataFrame(columns=['Model', 'Non-tuning train accuracy %', 'Tuning train accuracy %', 'Non-tuning test accuracy %', 'Tuning test accuracy %'])\n",
    "lr_result = pd.DataFrame(data=[['Logistic regression', ..., ..., ..., ...]],\n",
    "                        columns=['Model', 'Non-tuning train accuracy %', 'Tuning train accuracy %', 'Non-tuning test accuracy %', 'Tuning test accuracy %'])\n",
    "result = result.append(lr_result, ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27999c6c-95b7-4ea9-ad6b-333a012255df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
